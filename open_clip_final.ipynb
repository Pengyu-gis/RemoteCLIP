{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMsQTwAdO8zRrOQmUkwoOxi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pengyu-gis/RemoteCLIP/blob/main/open_clip_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 准备数据与环境\n",
        "使用的数据是Flickr 8k Dataset\n",
        "数据集地址: https://www.kaggle.com/datasets/adityajn105/flickr8k/code"
      ],
      "metadata": {
        "id": "_uGN9CntLW-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 上传 kaggle.json\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 确保 kaggle.json 被正确上传\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# 创建 kaggle 目录并移动 kaggle.json 到该目录\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "# 更改权限\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "acYmflNWInAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "!unzip flickr8k.zip"
      ],
      "metadata": {
        "id": "B1dycCBgJdCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open_clip_torch"
      ],
      "metadata": {
        "id": "CE6xMKXtJuek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 加载模型\n",
        "使用open_clip提供的接口来加载预训练的CLIP模型。可以选择一个适合您任务的模型版本"
      ],
      "metadata": {
        "id": "zln-LK2RJ3xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import open_clip\n",
        "\n",
        "# 加载预训练模型\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')"
      ],
      "metadata": {
        "id": "NNOUS0lSJ0jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定义数据加载器\n",
        "为了能够加载TIFF图像和对应的文本描述, 需要定义一个自定义的torch.utils.data.Dataset。以下是一个示例实现:"
      ],
      "metadata": {
        "id": "r2yJjXKMJ76X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None):\n",
        "        # 使用pandas读取文本文件，假设字段之间是由逗号分隔的\n",
        "        self.img_labels = pd.read_csv(annotations_file, delimiter=',')\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # 读取JPG文件\n",
        "        caption = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, caption\n",
        "\n",
        "# 设置数据转换\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# 创建数据集和数据加载器\n",
        "dataset = ImageTextDataset(annotations_file='/content/captions.txt',\n",
        "                           img_dir='/content/Images',\n",
        "                           transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "5E5h7EGCKC9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 微调模型\n",
        "一旦定义了数据加载器，就可以开始微调模型。这涉及到迭代数据加载器，将每批图像和文本送入模型，计算损失，并更新模型的权重。以下是微调过程的一个简化示例:"
      ],
      "metadata": {
        "id": "EXrmGcrPMGyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "from open_clip import tokenize\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# If the model isn't automatically moved to the correct device, explicitly do so\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# 假设已经定义了optimizer和loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3  # Example number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, captions in dataloader:  # Assuming dataloader is your DataLoader instance\n",
        "        images = images.to(\"cuda\")\n",
        "        text_tokens = tokenize(captions).to(\"cuda\")  # Ensure captions are properly processed if needed\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Temporarily capture the entire output\n",
        "        output = model(images, text_tokens)\n",
        "        # print(type(output))  # Check the type of the output\n",
        "        # print(len(output))   # If it's a tuple or list, check how many elements it contains\n",
        "        image_features, text_features, _ = model(images, text_tokens)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(image_features, text_features)  # Placeholder, adjust as necessary\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}\")\n"
      ],
      "metadata": {
        "id": "PPw6uUZtMJUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}